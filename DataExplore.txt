## 데이터 살펴보기
# 데이터 불러오기. column 명 살펴보기
엑셀 시트 켜고 Variable(Name) / Type / segment / Expectation / conclusion / Comments 
Variable - Variable name.
Type - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.
Segment - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').
Expectation - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.
Conclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.
Comments - Any general comments that occured to us.
작성하기(시간이 오래 걸리지만 꼭 해야할 작업)

## 데이터 표준화와 정규화
# 데이터 분석에서의 Log의 중요성 - 데이터 분포를 조정하는 방법
데이터 분석에서 Log를 취하는 이유는 '정규성을 높이고 분석(회귀분석 등)에서의 정확한 값을 얻기 위함'
데이터 간 편차를 줄여 왜도(퍼져있는 정도)와 첨도(뾰족한 정도)를 줄일 수 있기 때문에 정규성이 높아진다. 
예를 들어 연령 같은 경우 숫자의 범위가 0~120세 이하, 재산 보유액의 경우에는 0원~ 무한한 값이 가능.
데이터 간의 단위가 달라지면 결과값이 이상해질 수 있음. 
=======================================
그래서 로그의 역할은 큰 수를 같은 비율의 작은 수로 바꿔주는 것
로그는 큰 수를 작게 만들고 복잡한 계산을 간편하게 하기 위해 사용.
예를 들어 100=10^2 임. 100에 상용로그를 취하면 2로 작아짐. 또한 로그 성질에 의하여 곱하기는 더하기로, 나누기는 빼기로
되버리니 값이 작아진다.
+ 숫자가 원래 작으면 로그 처리를 해도 거의 변하지 않을 수 있다.
=======================================
큰 수를 작게 만들고, 복잡한 계산을 쉽게 만들고, 왜도와 첨도를 줄여서 데이터 분석 시 의미있는 결과 도출을 위함.
지수함수와 로그함수를 그려보기만 해도 쉽게 이해가 가능하다.

Log / 루트 / 역수의 역할 -> 큰 수를 축소시킨다. 계산을 간편하게 만들어준다.

* 독립변수가 몇 % 변할 때, 종속변수가 몇 % 변하는지 변화량의 비율로 보고자 할 때 독립변수와 종속변수에 모두 자연로그를
	취해준다.  --> 범주형 자료분석에서 오즈비를 생각해보자. 맞나?
독립변수에만 자연로그를 취해주면 회귀계수가 의미하는 것은 독립변수가 1% 변할 때, 종속변수를 얼만큼(비율x 양o)
	변하는가를 알 수 있다.
종속변수에만 자연로그를 취하면 회귀계수가 의미하는 것은 독립변수가 한 단위 변화할때, 종속변수를 몇 % 변화하는가를
	나타낸다.
일반적으로 자연로그를 씌우면 비선형관계를 선형관계로 변환할 수 있다고 한다. 특히 회귀분석에서는 x y가 선형관계라는
	정의가 있기 때문에 회귀분석을 위해 많이 해준다.
=======================================
작은 숫자로 만들려고 / 독립변수와 종속변수 변화관계에서 절대량이 아닌 비율을 보려고 / 비선형을 선형으로 만들기 위해
로그 변환을 하면 상대적으로 작은 값에 몰려 있는 데이터의 모양이 퍼지면서 좌우대칭 형태로 분포의 모양이 달라진다.
http://datum.io/tag/log/
* raw 데이터에 로그 변환이 너무 과해서 좌편향 데이터가 우편향이 되어버렸다면 raw 데이터에 대신 루트를 씌워보자.
* 데이터에 0이 포함되어 있으면 아주 작은 수를 모든 값에 더해주는 것이 변환 작업에 유리하다. 특히 로그 변환을 한다면
	필수적인 작업이다. 보통 0이 있으면 1을 더해서 변환을 시작한다. Log0이 정의되지 않기 때문. 

* -math 모듈 math.exp(x) / math.log(x) / math.log10(x)  -> x 에는 숫자가 들어가는데 다른게 들어갈 수 있나?
* 
=======================================

# 변수 스케일링(표준화) - 평균을 기준으로 어느 정도 떨어져 있는지를 나타내주는 기능
https://mkjjo.github.io/python/2019/01/10/scaler.html + Reference 도 참고할 것
변수의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우, 스케일링으로 해결한다.
다중공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 변수 선택이나 PCA 를 사용한 차원 축소 등으로 해결한다.
예를 들어, 집값 예측을 위한 회귀모형을 만드는데, 지하철역과의 거리(km) / 층수 / 관리비 / 화장실 개수 등이 들어간다고
해보자. 거리는 0~20(km) 값을 갖고 층수는 1~20(층), 관리비는 100,000 ~ 500,000(만원), 화장실 개수는 1~3(개) 를 갖는다면 변수들의 단위를
변환하지 않고 그대로 만든다면 관리비가 주는 영향이 가장 높게 나타날 것이다. 그렇기 때문에 단위에 대한 스케일링을
반드시 진행해주고 해야한다.
스케일링(표준화)과 로그/루트/역수 취하는 것은 다르다. 표준화는 단위 차이를 통일시켜주는 것이고 로그는 데이터를
정규분포 형식으로 만들어주기 위한 것이다.

=======================================

변수들을 그대로 사용하면 모델이 정확하지 않게 나옴.(예측 오차가 커지게 된다.)
그리고 변환된 변수를 사용하면 회귀 성능이 향상될 수도 있다.
파이썬 scikit-Learn 에서 제공하는 것 중 대표적인 4가지는
* Standard Scaler - 평균을 제거하고 데이터를 분산 단위로 조정. 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된
	데이터의 확산이 매우 달라질 수 있음.
* MinMax Scaler - 모든 feature의 값이 0~1 사이에 있도록 데이터를 재조정. 이상치가 있는 경우 변환된 값이 매우 좁은
	범위로 압축될 수 있다. 역시 이상치에 민감하다.
* MaxAbs Scaler - 절대값이 0~1 사이에 매핑되도록 한다. -1 ~ 1 사이로 재조정한다. 양수 데이터로만 구성된 특징 데이터셋에서는
	MinMax Scaler와 유사하게 동작하며, 큰 이상치에 민감할 수 있다.
* Robust Scaler - 아웃라이어의 영향을 최소화한 기법. 중앙값과 IQR을 사용하기 때문에 Std와 비교해보면 표준화 후
	동일한 값을 더 넓게 분포 시키고 있을을 확인할 수 있다.
=======================================

모든 스케일러 처리 전에는 아웃라이어 제거가 선행되어야 한다. 또한 데이터의 분포 특징에 따라 적절한 스케일러 적용必
* Robust / Standard / MinMax 순으로 성능이 괜찮은 것 같다.
* Python 에 scikit-learn 에서는 변환한 결과를 반영해주는 함수도 존재한다. fit_transform

=======================================

# 데이터 정규화
데이터의 범위를 0과 1로 변환하며 데이터의 분포를 조정하는 방법 (로그랑은 또 다른 기능)
0~1 사이로 만들어서 잘보이게 하려고, 평균 0 아님. 표준화는 평균이 0이고 표준편차가 1로 만들어줌.(Std 일때)

=======================================

로그/루트/분수 : 큰 수를 작은 수로 바꿔주는 것. 분포가 잘 보이도록, 큰 수를 축소시켜서 계산이 간편하도록
		자산이 0원부터 100억이면 100억에 루트 씌우면 확 끌어올 수 있음.
		하지만 작은 수들끼리 모여있을 때는 어떤 방식을 쓰는지 모르겠음. 루트를 씌워도 소용이 없음.
		좌/우편향 데이터를 정규분포로 만들어줄 수 있음. -> 잘 생각해보면 그렇게 된다는거 이해 가능.
표준화(standardization) : 평균이 0, 표준편차 1로 만들어줌. 평균을 기준으로 얼마나 떨어져있는지를 보여줌.
			나오기 힘든 값인지 아니면 흔한 값인지, 방식에 따라 평균0, 표준편차 1이 아닐수도 있음.
			데이터들이 단위가 모두 다를 때 하면 된다. x축을 압축
			정규분포로 만들어주지 않음. 그냥 숫자만 바꿔줌. 결국 분포도 일정하다.
정규화(normalization) : 그냥 값을 0~1 사이로 만들어줘서 잘 보이도록. 평균 0이 아님. 평균 기준 없음. y축을 압축

이걸 하는 이유 -> 모델을 만들건데, 데이터가 편향되어있으면 정확한 모델이 만들어지지 않음. 과적합/편향적 모델/
		예외를 찾아내지 못함. 모델의 정확도를 높이기 위한 방법임.
+ 모델의 정확도를 높이기 위해서는 전처리한 데이터를 샘플링을 어떻게 하느냐도 중요함. 뒤에서 다룰 내용

* 로그/루트/분수를 취했음에도 그래프가 좌편향/우편향이 움직이지 않는다면???
아마도 데이터의 전반적인 값 자체가 큰 값이 없어서 작아질 값이 없어서 그대로 나오는 것이라고 생각된다.
왜냐면 큰 값을 작게 해주는 것인데 이미 작은 값이면 작아질 것이 없기 때문에 있는 그대로 나올 것이라고 생각.

=======================================
# 통계
pdf(Probability Density Function, 확률 밀도 함수)와 
cdf(cumulative distribution function, 누적 분포 함수)가 그것이다.
pdf는 cdf의 도함수다.
